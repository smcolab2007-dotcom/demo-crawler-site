# demo-crawler-site
A simple educational demo showing how to build and test a basic web page and an ethical web crawler. Designed for teaching and learning purposes. Follow ethical practices: crawl only allowed sites, respect robots.txt, and never collect or share private data.


## ğŸš€ Project Structure

demo-crawler-site/ â”œâ”€â”€ index.html        â†’ Public page (OK to crawl) â”œâ”€â”€ private.html      â†’ Private page (blocked by robots.txt) â”œâ”€â”€ mixed.html        â†’ Mixed content (public + hidden section) â”œâ”€â”€ robots.txt        â†’ Controls crawler behavior â””â”€â”€ polite_crawler.py â†’ Example polite crawler (for your own test site)

---




*** Instructions to students:
## âš–ï¸ Safety & Ethical Notes

This project is **for learning only**.

1. âœ… Crawl only your own test site or sites that **explicitly allow** crawling.  
2. ğŸš« Never collect, store, or share private or personal data.  
3. ğŸ”’ Do **not** bypass authentication or access restricted content.  
4. ğŸ¤– Always follow `robots.txt` rules and **respect website permissions**.  

---

## ğŸ§­ How to Use

1. Open the repository in GitHub Pages (enable it under **Settings â†’ Pages**).  
2. Run `polite_crawler.py` in Google Colab or your local Python environment.  
3. Observe how the crawler politely checks `robots.txt` and avoids restricted pages.

---

## ğŸ“˜ License

This demo is released for **educational use only** â€” not for production or commercial crawling.
