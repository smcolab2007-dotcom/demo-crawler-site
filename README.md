# demo-crawler-site
A simple educational demo showing how to build and test a basic web page and an ethical web crawler. Designed for teaching and learning purposes. Follow ethical practices: crawl only allowed sites, respect robots.txt, and never collect or share private data.


## ğŸš€ Project Structure

demo-crawler-site/ â”œâ”€â”€ index.html        â†’ Public page (OK to crawl) â”œâ”€â”€ private.html      â†’ Private page (blocked by robots.txt) â”œâ”€â”€ mixed.html        â†’ Mixed content (public + hidden section) â”œâ”€â”€ robots.txt        â†’ Controls crawler behavior â””â”€â”€ polite_crawler.py                 â† fixed URL version
â””â”€â”€ polite_crawler_input_url.py       â† runtime input version (recommended for students)

---




*** Instructions to students:
## âš–ï¸ Safety & Ethical Notes

This project is **for learning only**.

1. âœ… Crawl only your own test site or sites that **explicitly allow** crawling.  
2. ğŸš« Never collect, store, or share private or personal data.  
3. ğŸ”’ Do **not** bypass authentication or access restricted content.  
4. ğŸ¤– Always follow `robots.txt` rules and **respect website permissions**.  

---

âš ï¸ Safety Note â€” Ethical Crawling

This crawler is for learning only. Use it only on your own GitHub Pages sites or other pages that explicitly allow crawling in their robots.txt.

âŒ Do not use it on:

.Google (google.com)

.Facebook (facebook.com)

.Twitter/X (twitter.com)

.Instagram (instagram.com)

.LinkedIn (linkedin.com)

.Any site that disallows bots or needs login


These platforms block automated access and track bot traffic for security. Using a crawler there violates their Terms of Service and data protection laws.

âœ… Safe practice:
Replace the seed URL only with your own public demo repo (for example,
https://your-username.github.io/your-demo-repo/).




## ğŸ§­ How to Use

1. Open the repository in GitHub Pages (enable it under **Settings â†’ Pages**).  
2. Run `polite_crawler.py` in Google Colab or your local Python environment.  
3. Observe how the crawler politely checks `robots.txt` and avoids restricted pages.
4. Optional: Run `polite_crawler_visible.py` in Google Colab or your local Python environment. Observe the Output.

## ğŸ§‘â€ğŸ“ For Students â€” How to Run in Google Colab

# STEP 1 â€” Get your instructor's GitHub repo
!git clone https://github.com/smcolab2007-dotcom/demo-crawler-site.git
%cd demo-crawler-site

# STEP 2 â€” Install required packages
!pip install requests beautifulsoup4 --quiet

# STEP 3 â€” Run the crawler
!python polite_crawler_unfinished.py

---
## SNA PART:
 Run `sna_polite_crawler_unfinished.py`

## Metrics Calculations:
 Run `sna_metrics_polite_crawler_unfinished.py`

## ğŸ“˜ License

This demo is released for **educational use only** â€” not for production or commercial crawling.
